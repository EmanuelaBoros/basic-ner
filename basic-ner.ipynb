{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyETdB-dkBsX"
   },
   "source": [
    "## **Fine-tuning BERT for named-entity recognition**\n",
    "\n",
    "In this notebook, we are going to use [**AutoModelForTokenClassification**](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForTokenClassification) which is included in the [Transformers library](https://github.com/huggingface/transformers) by HuggingFace. This model has a Transformer as its base architecture, with a token classification head on top, allowing it to make predictions at the token level, rather than the sequence level. **Named entity recognition (NER)** is typically treated as a token classification problem, so that's what we are going to use it for.\n",
    "\n",
    "This tutorial uses the idea of **transfer learning**, i.e. first pretraining a large neural network in an unsupervised way, and then fine-tuning that neural network on a task of interest. In this case, BERT is a neural network pretrained on t tasks: masked language modeling and next sentence prediction. Now, we are going to fine-tune this network on a NER dataset. Fine-tuning is supervised learning, so this means we will need a labeled dataset.\n",
    "\n",
    "If you want to know more about BERT, I suggest the following resources:\n",
    "* the original [paper](https://arxiv.org/abs/1810.04805)\n",
    "* Jay Allamar's [blog post](http://jalammar.github.io/illustrated-bert/) as well as his [tutorial](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n",
    "* Chris Mccormick's [Youtube channel](https://www.youtube.com/channel/UCoRX98PLOsaN8PtekB9kWrw)\n",
    "* Abbishek Kumar Mishra's [Youtube channel](https://www.youtube.com/user/abhisheksvnit)\n",
    "\n",
    "The following notebook largely follows the same structure as the tutorials by Abhishek Kumar Mishra. For his tutorials on the Transformers library, see his [Github repository](https://github.com/abhimishra91/transformers-tutorials).\n",
    "\n",
    "> NOTE: this notebook assumes basic knowledge about deep learning, BERT, and native PyTorch. If you want to learn more Python, deep learning and PyTorch, I highly recommend [cs224n](https://web.stanford.edu/class/cs224n/) by Stanford University and the [FastAI course](https://course.fast.ai/) by Jeremy Howard et al. Both are freely available on the web.  \n",
    "\n",
    "Now, let's move on to the real stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37354,
     "status": "ok",
     "timestamp": 1760619188814,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "IEnlUbgm8z3B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1760619188845,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "Sm1krxJtKxpx",
    "outputId": "bd74a004-143d-44b4-9171-45a20fd3cc62",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahwMsmyG5ZPE"
   },
   "source": [
    "#### **Downloading and preprocessing the data**\n",
    "Named entity recognition (NER) uses a specific annotation scheme, which is defined (at least for European languages) at the *word* level. An annotation scheme that is widely used is called **[IOB-tagging](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)**, which stands for Inside-Outside-Beginning. Each tag indicates whether the corresponding word is *inside*, *outside* or at the *beginning* of a specific named entity. The reason this is used is because named entities usually comprise more than 1 word.\n",
    "\n",
    "Let's have a look at an example. If you have a sentence like \"Barack Obama was born in HawaÃ¯\", then the corresponding tags would be   [B-PERS, I-PERS, O, O, O, B-GEO]. B-PERS means that the word \"Barack\" is the beginning of a person, I-PERS means that the word \"Obama\" is inside a person, \"O\" means that the word \"was\" is outside a named entity, and so on. So one typically has as many tags as there are words in a sentence.\n",
    "\n",
    "So if you want to train a deep learning model for NER, it requires that you have your data in this IOB format (or similar formats such as [BILOU](https://stackoverflow.com/questions/17116446/what-do-the-bilou-tags-mean-in-named-entity-recognition)). There exist many annotation tools which let you create these kind of annotations automatically (such as Spacy's [Prodigy](https://prodi.gy/), [Tagtog](https://docs.tagtog.net/) or [Doccano](https://github.com/doccano/doccano)). You can also use Spacy's [biluo_tags_from_offsets](https://spacy.io/api/goldparse#biluo_tags_from_offsets) function to convert annotations at the character level to IOB format.\n",
    "\n",
    "Here, we will use [HIPE](https://github.com/hipe-eval/HIPE-2022-data) that is already in IOB format. Uncomment to clone it in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:hipe-eval/HIPE-2022-data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = \"HIPE-2022-data/data/v2.1/hipe2020/fr/\"\n",
    "\n",
    "train_path = dev_path = test_path = None\n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    if \"train\" in file:\n",
    "        train_path = os.path.join(data_path, file)\n",
    "    elif \"dev\" in file:\n",
    "        dev_path = os.path.join(data_path, file)\n",
    "    elif \"test\" in file:\n",
    "        test_path = os.path.join(data_path, file)\n",
    "\n",
    "print(\"Train path:\", train_path)\n",
    "print(\"Dev path:\", dev_path)\n",
    "print(\"Test path:\", test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Training Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the NER model, we define several key parameters that control how the model learns.\n",
    "\n",
    "| Parameter | Description |\n",
    "|------------|-------------|\n",
    "| **`MAX_LEN`** | Maximum sequence length for tokenized inputs. Historical documents may contain long sentences, so values like 256 or 512 often work better than 128. |\n",
    "| **`TRAIN_BATCH_SIZE` / `VALID_BATCH_SIZE`** | Number of samples processed per GPU step during training and validation. Adjust based on GPU memory. |\n",
    "| **`EPOCHS`** | Number of complete passes through the training dataset. 1 is for testing the setup; increase to 3â€“5 for real training. |\n",
    "| **`LEARNING_RATE`** | Step size for updating model weights. Smaller rates (e.g., `2e-5`) help stabilize training for small or noisy datasets. |\n",
    "| **`MAX_GRAD_NORM`** | Gradient clipping threshold to prevent exploding gradients in transformer models. |\n",
    "| **`BASE_MODEL`** | Pretrained multilingual model adapted for historical text â€” here we use [`dbmdz/bert-base-historic-multilingual-cased`](https://huggingface.co/dbmdz/bert-base-historic-multilingual-cased). Other options like `xlm-roberta-base` or `bert-base-multilingual-uncased` can also be tested. |\n",
    "\n",
    "Finally, we initialize the **tokenizer**, which splits text into tokens compatible with the base modelâ€™s vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MAX_LEN = 128 # might be a bit small, try 256, 512\n",
    "TRAIN_BATCH_SIZE = 32 # 4\n",
    "VALID_BATCH_SIZE = 32 # 2\n",
    "EPOCHS = 1 # increase until 3 or 5\n",
    "LEARNING_RATE = 2e-5 # 1e-05 -> For small datasets, you might even try 2e-5 or 3e-5.\n",
    "MAX_GRAD_NORM = 10\n",
    "\n",
    "BASE_MODEL = 'dbmdz/bert-base-historic-multilingual-cased' # xlm-roberta-base, bert-base-multilingual-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "sentence = \"NapolÃ©on Bonaparte est nÃ© Ã  Ajaccio en 1769.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert to input IDs (numerical representation)\n",
    "inputs = tokenizer(sentence, max_length=MAX_LEN, truncation=True, return_tensors=\"pt\")\n",
    "print(\"\\nInput IDs:\", inputs[\"input_ids\"])\n",
    "\n",
    "# Decode back to readable text (for verification)\n",
    "decoded = tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "print(\"\\nDecoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a token classification model (like NER), each entity label (e.g., B-PER, I-LOC, O, etc.) needs to be mapped to a unique integer ID that the model uses internally.\n",
    "This mapping is stored in the label_map, for example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"O\": 0,\n",
    "  \"B-PER\": 1,\n",
    "  \"I-PER\": 2,\n",
    "  \"B-LOC\": 3,\n",
    "  \"I-LOC\": 4\n",
    "}\n",
    "```\n",
    "When you first build the training dataset, you generate this mapping from the labels that appear in the training data.\n",
    "Saving it ensures that this mapping stays consistent across all splits (train, dev, test) and across future runs.\n",
    "\n",
    "If you rerun the script or perform inference later, you load the same label_map from disk.\n",
    "This guarantees that:\n",
    "* The label IDs mean the same thing as during training.\n",
    "* The model outputs (which are integers) can be correctly converted back to label names.\n",
    "* You avoid random or inconsistent label orderings that could break evaluation or inference.\n",
    "    \n",
    "If you didn't save the label_map, it would be recreated dynamically (possibly in a different order) when loading the test or inference dataset. That would cause a mismatch like:\n",
    "* 1 = B-PER during training\n",
    "* 1 = B-ORG during inference\n",
    "\n",
    "...making your predictions meaningless or incorrect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tricky part of NER with BERT is that BERT relies on **wordpiece tokenization**, rather than word tokenization. This means that we should also define the labels at the wordpiece-level, rather than the word-level!\n",
    "\n",
    "For example, if you have word like \"Washington\" which is labeled as \"B-GPE\", but it gets tokenized to \"Wash\", \"##ing\", \"##ton\", then one approach could be to handle this by only train the model on the tag labels for the first word piece token of a word (i.e. only label \"Wash\" with \"B-GPE\"). This is what was done in the original BERT paper, see Github discussion [here](https://github.com/huggingface/transformers/issues/64#issuecomment-443703063).\n",
    "\n",
    "Note that this is a **design decision**. You could also decide to propagate the original label of the word to all of its word pieces and let the model train on this. In that case, the model should be able to produce the correct labels for each individual wordpiece. This was done in [this NER tutorial with BERT](https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L118). Another design decision could be to give the first wordpiece of each word the original word label, and then use the label â€œXâ€ for all subsequent subwords of that word. All of them seem to lead to good performance.\n",
    "\n",
    "Below, we define a regular PyTorch [dataset class](https://pytorch.org/docs/stable/data.html) (which transforms examples of a dataframe to PyTorch tensors) which you can find in `dataset.py`. Here, each sentence gets tokenized, the special tokens that BERT expects are added, the tokens are padded or truncated based on the max length of the model, the attention mask is created and the labels are created based on the dictionary which we defined above. Word pieces that should be ignored have a label of -100 (which is the default `ignore_index` of PyTorch's [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)).\n",
    "\n",
    "For more information about BERT's inputs, see [here](https://huggingface.co/transformers/glossary.html).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import NewsDataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "output_folder = \"experiments\"\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Output folder ready: {output_folder}\")\n",
    "\n",
    "label_map_path = os.path.join(output_folder, \"label_map.json\")\n",
    "\n",
    "if not os.path.exists(label_map_path):\n",
    "    print(f\"Label map does not exist in {label_map_path}. Creating it now.\")\n",
    "    train_dataset = NewsDataset(\n",
    "        tsv_dataset=train_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    label_map = train_dataset.get_label_map()\n",
    "\n",
    "    dev_dataset = NewsDataset(\n",
    "        tsv_dataset=dev_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        label_map=label_map,\n",
    "    )\n",
    "\n",
    "    label_map = dev_dataset.get_label_map()\n",
    "\n",
    "    test_dataset = NewsDataset(\n",
    "        tsv_dataset=test_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        label_map=label_map,\n",
    "    )\n",
    "\n",
    "    label_map = test_dataset.get_label_map()\n",
    "\n",
    "    # Save the label map\n",
    "    with open(label_map_path, \"w\") as f:\n",
    "        json.dump(label_map, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Loading existing label map from {label_map_path}.\")\n",
    "    with open(label_map_path, \"r\") as f:\n",
    "        label_map = json.load(f)\n",
    "\n",
    "    # Initialize datasets with the loaded label map\n",
    "    train_dataset = NewsDataset(\n",
    "        tsv_dataset=train_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        label_map=label_map,\n",
    "    )\n",
    "\n",
    "    dev_dataset = NewsDataset(\n",
    "        tsv_dataset=dev_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        label_map=label_map,\n",
    "    )\n",
    "\n",
    "    test_dataset = NewsDataset(\n",
    "        tsv_dataset=test_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        label_map=label_map,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a single label type: NE-COARSE-LIT\n",
    "\n",
    "For simplicity, we will only train a model on NE-COARSE-LIT, which represents the coarse-grained named entity categories in the HIPE dataset (e.g., PER, LOC, ORG, etc.).\n",
    "\n",
    "The dataset actually contains multiple annotation layers such as:\n",
    "* NE-COARSE-LIT\n",
    "* NE-COARSE-METO\n",
    "* NE-FINE-LIT\n",
    "* NE-FINE-METO\n",
    "* NE-FINE-COMP\n",
    "* NE-NESTED\n",
    "\n",
    "â€¦but training a multi-output model for all of them simultaneously would require a more complex setup.\n",
    "So, for now, we focus on one layer to illustrate the process clearly.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map['NE-COARSE-LIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2131,
     "status": "ok",
     "timestamp": 1760619201272,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "deLB9HVX5I6F"
   },
   "outputs": [],
   "source": [
    "# Print one example from the train dataset\n",
    "example = train_dataset[0]\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map['NE-COARSE-LIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsjhdQbE-Lve"
   },
   "source": [
    "We create two dictionaries: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1760619203257,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "CFRDM8WsQXvL",
    "outputId": "106ed924-4eb1-4111-da55-ad3fa27fb2b5"
   },
   "outputs": [],
   "source": [
    "# Map label names to IDs\n",
    "labels_to_ids = label_map['NE-COARSE-LIT']\n",
    "\n",
    "# Reverse mapping: IDs to label names\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "print(\"Label â†’ ID:\", labels_to_ids)\n",
    "print(\"ID â†’ Label:\", ids_to_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8obZumRTBrT"
   },
   "source": [
    "Let's look at how one example from the test dataset can be decoded to check if the labels align with the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one example\n",
    "example = test_dataset[1]\n",
    "\n",
    "# Decode input tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(example[\"input_ids\"])\n",
    "\n",
    "# Select the label set\n",
    "label_key = \"NE-COARSE-LIT\"\n",
    "label_ids = example[\"token_targets\"][label_key]\n",
    "\n",
    "# Load label map\n",
    "labels_to_ids = label_map[label_key]\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "# Decode the labels\n",
    "decoded_labels = [ids_to_labels.get(int(label_id), \"PAD\") if label_id != -100 else \"IGNORED\"\n",
    "                  for label_id in label_ids]\n",
    "\n",
    "# Print tokens and labels side-by-side for easy inspection\n",
    "for token, label in zip(tokens, decoded_labels):\n",
    "    print(f\"{token:15} {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPYV2Ld6Tr5I"
   },
   "source": [
    "What this shows:\n",
    "* Converts token IDs â†’ readable tokens ([CLS], Le, public, est, etc.)\n",
    "* Converts label IDs â†’ readable labels (O, B-pers, etc.)\n",
    "* Skips ignored tokens (-100), which correspond to padding or special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ky68FcTgWnfN"
   },
   "source": [
    "Now, let's define the corresponding PyTorch dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1760619229392,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "KIw793myWOmi"
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_params)\n",
    "test_loader = DataLoader(test_dataset, **test_params)\n",
    "dev_loader = DataLoader(dev_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73OzU7oXRxR8"
   },
   "source": [
    "#### **Defining the BERT Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-iGhnhdLNdP"
   },
   "source": [
    "Here we define the model, BertForTokenClassification, and load it with the pretrained weights of \"bert-base-uncased\". The only thing we need to additionally specify is the number of labels (as this will determine the architecture of the classification head).\n",
    "\n",
    "Note that only the base layers are initialized with the pretrained weights. The token classification head of top has just randomly initialized weights, which we will train, together with the pretrained weights, using our labelled dataset. This is also printed as a warning when you run the code cell below.\n",
    "\n",
    "Then, we move the model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795,
     "referenced_widgets": [
      "b7fc706b45d8403c9a9cd9ebdc080ed6",
      "f5abbdd3c4114085a433441acad3fcf9",
      "6ee430895b3444fabdc8aff0dda894c7",
      "26d574208294422195d9e205d9af82d9",
      "72aa45c0557d4e08b494ecb2ff4e839c",
      "24746b0390f84a2ab93b41cb7c88ccd2",
      "78793edc197b46b29f8556c8f8449bb4",
      "66001ea919cf4906af57a7017f6f209b",
      "7e355666d5624dcabb4925e2414cdc2c",
      "44e5a463448b44a2a6e4cde07dc86fdb",
      "22c9962b08264b18a4c1e59b63ad433c"
     ]
    },
    "executionInfo": {
     "elapsed": 22878,
     "status": "ok",
     "timestamp": 1760619256459,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "cB9MR3KcWXUs",
    "outputId": "f5f797ce-ef56-4efb-8ccd-f54b790db585",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, num_labels=len(labels_to_ids))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp7Yl4JyWhDj"
   },
   "source": [
    "#### **Training the BERT Model**\n",
    "\n",
    "Before training the model, let's perform a sanity check, which I learned thanks to Andrej Karpathy's wonderful [cs231n course](http://cs231n.stanford.edu/) at Stanford (see also his [blog post about debugging neural networks](http://karpathy.github.io/2019/04/25/recipe/)). The initial loss of your model should be close to -ln(1/number of classes) = -ln(1/17) = 2.83.\n",
    "\n",
    "Why? Because we are using cross entropy loss. The cross entropy loss is defined as -ln(probability score of the model for the correct class). In the beginning, the weights are random, so the probability distribution for all of the classes for a given token will be uniform, meaning that the probability for the correct class will be near 1/17. The loss for a given token will thus be -ln(1/17). As PyTorch's [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (which is used by `BertForTokenClassification`) uses *mean reduction* by default, it will compute the mean loss for each of the tokens in the sequence for which a label is provided.\n",
    "\n",
    "Let's verify this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1760619260299,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "eqAN7YVIjKTr",
    "outputId": "98eb6e5b-b531-4931-bf06-ddffec4e76bb"
   },
   "outputs": [],
   "source": [
    "inputs = train_dataset[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs['token_targets']['NE-COARSE-LIT'].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLdwsru9Mh7U"
   },
   "source": [
    "This looks good. Let's also verify that the logits of the neural network have a shape of (batch_size, sequence_length, num_labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1760619262265,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "X-z6YCpGnvfj",
    "outputId": "bbf27cd6-80ba-4464-dbdd-1fb29fadb92a"
   },
   "outputs": [],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwDLXxOVOCvD"
   },
   "source": [
    "Next, we define the optimizer. Here, we are just going to use Adam with a default learning rate. One can also decide to use more advanced ones such as AdamW (Adam with weight decay fix), which is [included](https://huggingface.co/transformers/main_classes/optimizer_schedules.html) in the Transformers repository, and a learning rate scheduler, but we are not going to do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1760619264163,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "kznSQfGIWdU4"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZQ8JMF0NOe1"
   },
   "source": [
    "Now let's define a regular PyTorch training function. It is partly based on [a really good repository about multilingual NER](https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L344)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def train_one_epoch(epoch, model, training_loader, validation_loader, optimizer, device, ids_to_labels, eval_every=100):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    train_step_losses = []   # training loss every 100 steps\n",
    "    eval_step_losses = []    # validation loss every 100 steps\n",
    "\n",
    "    model.train()\n",
    "    progress_bar = tqdm(training_loader, desc=f\"Epoch {epoch} [Training]\", leave=False)\n",
    "\n",
    "    for idx, batch in enumerate(progress_bar):\n",
    "        # print(batch)\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['token_targets']['NE-COARSE-LIT'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # Accuracy\n",
    "        flattened_targets = labels.view(-1)\n",
    "        active_logits = logits.view(-1, model.num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "        active_mask = flattened_targets != -100\n",
    "\n",
    "        labels_filtered = torch.masked_select(flattened_targets, active_mask)\n",
    "        preds_filtered = torch.masked_select(flattened_predictions, active_mask)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(\n",
    "            labels_filtered.cpu().numpy(),\n",
    "            preds_filtered.cpu().numpy()\n",
    "        )\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # Gradient clipping + backward\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record train loss every 100 steps\n",
    "        if idx % 100 == 0:\n",
    "            avg_loss = tr_loss / nb_tr_steps\n",
    "            train_step_losses.append(avg_loss)\n",
    "\n",
    "            # Run quick evaluation every 100 steps\n",
    "            model.eval()\n",
    "            eval_loss, eval_steps = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for v_idx, v_batch in enumerate(validation_loader):\n",
    "                    if v_idx >= 5:  # limit evaluation to 5 batches to save time\n",
    "                        break\n",
    "                    v_ids = v_batch['input_ids'].to(device, dtype=torch.long)\n",
    "                    v_mask = v_batch['attention_mask'].to(device, dtype=torch.long)\n",
    "                    v_labels = v_batch['token_targets']['NE-COARSE-LIT'].to(device, dtype=torch.long)\n",
    "\n",
    "                    v_outputs = model(input_ids=v_ids, attention_mask=v_mask, labels=v_labels)\n",
    "                    eval_loss += v_outputs.loss.item()\n",
    "                    eval_steps += 1\n",
    "            model.train()\n",
    "\n",
    "            eval_step_losses.append(eval_loss / eval_steps)\n",
    "\n",
    "        # Update tqdm\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{tmp_tr_accuracy:.4f}'\n",
    "        })\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    epoch_acc = tr_accuracy / nb_tr_steps\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"  Training loss: {epoch_loss:.4f}\")\n",
    "    print(f\"  Training accuracy: {epoch_acc:.4f}\")\n",
    "    print(f\"  Last eval (step-level) loss: {eval_step_losses[-1]:.4f}\" if eval_step_losses else \"\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, train_step_losses, eval_step_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2dsCyP7dcF3"
   },
   "source": [
    "And let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_loss = float('inf')  # track best validation loss\n",
    "all_train_losses = []\n",
    "all_eval_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc, step_train_losses, step_eval_losses = train_one_epoch(\n",
    "        epoch, model, train_loader, dev_loader, optimizer, device, ids_to_labels, eval_every=100\n",
    "    )\n",
    "\n",
    "    all_train_losses.extend(step_train_losses)\n",
    "    all_eval_losses.extend(step_eval_losses)\n",
    "\n",
    "    # Compute mean validation loss for the epoch\n",
    "    epoch_eval_loss = step_eval_losses[-1] if step_eval_losses else None\n",
    "\n",
    "    # Save best model if validation improves\n",
    "    if epoch_eval_loss is not None and epoch_eval_loss < best_eval_loss:\n",
    "        best_eval_loss = epoch_eval_loss\n",
    "        model_save_path = os.path.join(output_folder, f\"best_model_epoch_{epoch}\")\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "        model.save_pretrained(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "        print(f\"âœ… New best model saved at epoch {epoch} with eval loss = {best_eval_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"No improvement at epoch {epoch}. Best eval loss so far: {best_eval_loss:.4f}\")\n",
    "\n",
    "# Optionally, save the loss history for plotting later\n",
    "with open(os.path.join(output_folder, \"training_history.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_losses\": all_train_losses,\n",
    "        \"eval_losses\": all_eval_losses\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"ðŸ“Š Training history saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there's any [underfitting/overfitting](https://www.geeksforgeeks.org/machine-learning/underfitting-and-overfitting-in-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(all_train_losses, label='Training loss (per 100 steps)')\n",
    "plt.plot(all_eval_losses, label='Eval loss (per 100 steps)', linestyle='--')\n",
    "plt.xlabel(\"Steps (x100)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Evaluation Loss over Steps\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4jcSOJr680a"
   },
   "source": [
    "#### **Evaluating the BERT Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYUTuOEUdfFJ"
   },
   "source": [
    "Now that we've trained our model, we can evaluate its performance on the held-out test set (which is 20% of the data). Note that here, no gradient updates are performed, the model just outputs its logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict(model, dataloader, device, ids_to_labels):\n",
    "    \"\"\"\n",
    "    Generate predictions and ground truth labels for a token classification model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained HuggingFace model (e.g., BertForTokenClassification)\n",
    "        dataloader: DataLoader providing batches with 'input_ids', 'attention_mask', and 'labels'\n",
    "        device: torch.device('cuda' or 'cpu')\n",
    "        ids_to_labels: dict mapping label IDs to label strings\n",
    "\n",
    "    Returns:\n",
    "        labels_list: List[List[str]] â€” true labels per sentence (excluding padding)\n",
    "        predictions_list: List[List[str]] â€” predicted labels per sentence (excluding padding)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    labels_list = []\n",
    "    predictions_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['token_targets']['NE-COARSE-LIT'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                sentence_labels = labels[i]               # (seq_len,)\n",
    "                sentence_logits = logits[i]               # (seq_len, num_labels)\n",
    "                sentence_preds = torch.argmax(sentence_logits, dim=-1)\n",
    "\n",
    "                # Filter out ignored tokens (-100)\n",
    "                active_mask = sentence_labels != -100\n",
    "                filtered_labels = sentence_labels[active_mask]\n",
    "                filtered_preds = sentence_preds[active_mask]\n",
    "\n",
    "                labels_list.append([\n",
    "                    ids_to_labels[id.item()] for id in filtered_labels\n",
    "                ])\n",
    "                predictions_list.append([\n",
    "                    ids_to_labels[id.item()] for id in filtered_preds\n",
    "                ])\n",
    "\n",
    "    return labels_list, predictions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER is evaluated at the **entity level**, not just by individual tokens.  \n",
    "That means a prediction is counted as **correct** only if:\n",
    "- The predicted entity type (e.g. `pers`, `loc`, `org`) is correct, **and**\n",
    "- The entity **span** (the full sequence of tokens forming the entity) exactly matches the true one.\n",
    "\n",
    "This ensures that partial matches (like predicting only â€œBarackâ€ in â€œBarack Obamaâ€) are not overcounted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760619323836,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "_HXw2WD463vQ"
   },
   "outputs": [],
   "source": [
    "labels, predictions = predict(model, test_loader, device, ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1760619325972,
     "user": {
      "displayName": "Kamal nour",
      "userId": "12510954555232312338"
     },
     "user_tz": -120
    },
    "id": "0jDNXrjr-6BW",
    "outputId": "15369f70-e448-4a8b-ef22-eb35f5efe7ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here:\n",
    "- `labels` â†’ list of true IOB tags (e.g. `[\"B-pers\", \"I-pers\", \"O\", ...]`)\n",
    "- `predictions` â†’ list of predicted IOB tags from the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXPyaXrW0EX3"
   },
   "source": [
    "Performance already seems quite good, but note that we've only trained for 1 epoch. An optimal approach would be to perform evaluation on a validation set while training to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on Other Samples\n",
    "\n",
    "This is a quick sanity check â€” to visually confirm that your model can tag entities correctly before running a full evaluation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: run prediction on a single sentence\n",
    "example_sentence = \"NapolÃ©on Bonaparte est nÃ© Ã  Ajaccio en 1769.\"\n",
    "encoding = tokenizer(\n",
    "    example_sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze()\n",
    "\n",
    "# Decode tokens and labels\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())\n",
    "predicted_labels = [ids_to_labels[p.item()] for p in predictions]\n",
    "\n",
    "# Display tokens and predicted labels (excluding padding and special tokens)\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    if token not in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]:\n",
    "        print(f\"{token:15} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct words and merge labels accordingly\n",
    "reconstructed_words = []\n",
    "reconstructed_labels = []\n",
    "\n",
    "current_word = \"\"\n",
    "current_label = \"\"\n",
    "\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    if token in [\"[PAD]\", \"[CLS]\", \"[SEP]\"]:\n",
    "        continue\n",
    "\n",
    "    if token.startswith(\"##\"):\n",
    "        # continuation of the previous word\n",
    "        current_word += token[2:]\n",
    "    else:\n",
    "        # start of a new word\n",
    "        if current_word:\n",
    "            reconstructed_words.append(current_word)\n",
    "            reconstructed_labels.append(current_label)\n",
    "        current_word = token\n",
    "        current_label = label\n",
    "\n",
    "# append the last word\n",
    "if current_word:\n",
    "    reconstructed_words.append(current_word)\n",
    "    reconstructed_labels.append(current_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely print reconstructed words with their predicted labels\n",
    "print(\"Reconstructed tokens and labels:\\n\")\n",
    "print(f\"{'TOKEN':15} {'PRED'}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for token, label in zip(reconstructed_words, reconstructed_labels):\n",
    "    print(f\"{token:15} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_tsv = os.path.join(output_folder, \"predictions.tsv\")\n",
    "\n",
    "with open(output_tsv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"TOKEN\", \"PRED\"])  # column headers\n",
    "    for token, label in zip(reconstructed_words, reconstructed_labels):\n",
    "        writer.writerow([token, label])\n",
    "\n",
    "print(f\"âœ… Predictions saved to {output_tsv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ordnSliSUaQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb",
     "timestamp": 1760107736438
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a5bdb532bdb4313ac0f9d529b8d95db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4334603873ee4adfb9c15e5cbdda547e",
       "IPY_MODEL_c0ecc64656114d428e9221d4c370f0b6",
       "IPY_MODEL_7146c41dd25a4216917713be40795358"
      ],
      "layout": "IPY_MODEL_7f538662d0f640a18589a9e67d93e8b7"
     }
    },
    "22c9962b08264b18a4c1e59b63ad433c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24746b0390f84a2ab93b41cb7c88ccd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26d574208294422195d9e205d9af82d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44e5a463448b44a2a6e4cde07dc86fdb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_22c9962b08264b18a4c1e59b63ad433c",
      "value": "â€‡672M/672Mâ€‡[00:21&lt;00:00,â€‡39.4MB/s]"
     }
    },
    "2774d1f8de184d44b397403649dcebe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "28c7b80d3a314d809a58b87ff9255376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_533fcb7ad9a54f5dae76c6bfbf83b214",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b7890fe73044c2db27f365bd0deeacb",
      "value": 48
     }
    },
    "31a01d5dc6974e27a4309824aab594b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36b4e60fa55d4ac290cfaf786fefd42b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "375df6331b1d468a963f56362301b928": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9502bab38c2243d690971707fa5bd015",
       "IPY_MODEL_cbd2abb334b74a6581640f27074dd915",
       "IPY_MODEL_44a4b29f175344fb8d7e9b2a40692028"
      ],
      "layout": "IPY_MODEL_bf0f217768814b78806013fce05177ec"
     }
    },
    "3e343705ab7c4876b9a5683148b27d60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e5edc6e46f34881bc4cfc7423e549ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4334603873ee4adfb9c15e5cbdda547e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62620e8c2a6742ddb71e7fa2acc1d53c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_44a5279e0a1b4a27bd0015fcac8983b4",
      "value": "config.json:â€‡100%"
     }
    },
    "44a4b29f175344fb8d7e9b2a40692028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57e1a5cc66474186a4dc178a55357707",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3e5edc6e46f34881bc4cfc7423e549ac",
      "value": "â€‡872k/872kâ€‡[00:00&lt;00:00,â€‡4.17MB/s]"
     }
    },
    "44a5279e0a1b4a27bd0015fcac8983b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44e5a463448b44a2a6e4cde07dc86fdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "533fcb7ad9a54f5dae76c6bfbf83b214": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55aaef3340244760aba5dea203793125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55f823d565634ad29b7487437a109880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57e1a5cc66474186a4dc178a55357707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62620e8c2a6742ddb71e7fa2acc1d53c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "642ef68cf1db4e04a20eb2c7da75bfbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "654d2a5466c74a1588be0e3cf2da15eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66001ea919cf4906af57a7017f6f209b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66c2f4d865944557a031acc0dcf12e15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc66927112d345aa9bc1359533845ad6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_eb5596b5c3244d22a08b4e8348f9b674",
      "value": "â€‡48.0/48.0â€‡[00:00&lt;00:00,â€‡5.52kB/s]"
     }
    },
    "6ee430895b3444fabdc8aff0dda894c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66001ea919cf4906af57a7017f6f209b",
      "max": 672247920,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e355666d5624dcabb4925e2414cdc2c",
      "value": 672247920
     }
    },
    "7146c41dd25a4216917713be40795358": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_654d2a5466c74a1588be0e3cf2da15eb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e5a0c83e2ebc4c198b16eeffc3304ce3",
      "value": "â€‡625/625â€‡[00:00&lt;00:00,â€‡51.6kB/s]"
     }
    },
    "72aa45c0557d4e08b494ecb2ff4e839c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7491c0d72dc64b8090eac931edb6c844": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78793edc197b46b29f8556c8f8449bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b7890fe73044c2db27f365bd0deeacb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e355666d5624dcabb4925e2414cdc2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f4193d982444513ab626e3c7732bb80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f538662d0f640a18589a9e67d93e8b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84a74b5be1e74f9fa4b11b567a1d1d37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87bd6ef941d04813a5bc428b29363265": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55f823d565634ad29b7487437a109880",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_36b4e60fa55d4ac290cfaf786fefd42b",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "8ec218c40d644a799006ca5b6e42973f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9502bab38c2243d690971707fa5bd015": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f55acba756cf4ad68358eac3bc3999db",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ba9c751909d5428c962277ada24b06ba",
      "value": "vocab.txt:â€‡100%"
     }
    },
    "b7fc706b45d8403c9a9cd9ebdc080ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5abbdd3c4114085a433441acad3fcf9",
       "IPY_MODEL_6ee430895b3444fabdc8aff0dda894c7",
       "IPY_MODEL_26d574208294422195d9e205d9af82d9"
      ],
      "layout": "IPY_MODEL_72aa45c0557d4e08b494ecb2ff4e839c"
     }
    },
    "ba9c751909d5428c962277ada24b06ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb4e2e1230e64df79f225566bc559f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_642ef68cf1db4e04a20eb2c7da75bfbb",
      "max": 1715180,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc6289257d1e410eb9f5dbc13d48d8e1",
      "value": 1715180
     }
    },
    "bf0f217768814b78806013fce05177ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0ecc64656114d428e9221d4c370f0b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7491c0d72dc64b8090eac931edb6c844",
      "max": 625,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e343705ab7c4876b9a5683148b27d60",
      "value": 625
     }
    },
    "c39b341f5b5240d4aa85e38549f6a706": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d00417eb3f71485c960cb831f5d97cbc",
       "IPY_MODEL_28c7b80d3a314d809a58b87ff9255376",
       "IPY_MODEL_66c2f4d865944557a031acc0dcf12e15"
      ],
      "layout": "IPY_MODEL_ce468ae5c02148eca6887f3b8098d8c6"
     }
    },
    "c6f872eb645a4b7c9c20a987816bb51b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_87bd6ef941d04813a5bc428b29363265",
       "IPY_MODEL_bb4e2e1230e64df79f225566bc559f70",
       "IPY_MODEL_f2e138547da74502b88d6b0672ccb1c7"
      ],
      "layout": "IPY_MODEL_84a74b5be1e74f9fa4b11b567a1d1d37"
     }
    },
    "cbd2abb334b74a6581640f27074dd915": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e90792be1eb944a48c8f57909c1230e2",
      "max": 871891,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2774d1f8de184d44b397403649dcebe5",
      "value": 871891
     }
    },
    "ce468ae5c02148eca6887f3b8098d8c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d00417eb3f71485c960cb831f5d97cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31a01d5dc6974e27a4309824aab594b1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_55aaef3340244760aba5dea203793125",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "dc6289257d1e410eb9f5dbc13d48d8e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc66927112d345aa9bc1359533845ad6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5a0c83e2ebc4c198b16eeffc3304ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e90792be1eb944a48c8f57909c1230e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb5596b5c3244d22a08b4e8348f9b674": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2e138547da74502b88d6b0672ccb1c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f4193d982444513ab626e3c7732bb80",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8ec218c40d644a799006ca5b6e42973f",
      "value": "â€‡1.72M/1.72Mâ€‡[00:00&lt;00:00,â€‡6.15MB/s]"
     }
    },
    "f55acba756cf4ad68358eac3bc3999db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5abbdd3c4114085a433441acad3fcf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24746b0390f84a2ab93b41cb7c88ccd2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_78793edc197b46b29f8556c8f8449bb4",
      "value": "model.safetensors:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
